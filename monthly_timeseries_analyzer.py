# -*- coding: utf-8 -*-
"""
æœˆåº¦æ—¶é—´åºåˆ—åˆ†æå™¨ - å®ç”¨ç‰ˆæœ¬
é¿å¼€å¤æ‚çš„å¹²æ”¯æ—¥è½¬æ¢ï¼ŒæŒ‰æœˆèšåˆæ•°æ®è¿›è¡Œåˆ†æ

ä¼˜åŠ¿ï¼š
1. ä¸ä¾èµ–ç²¾ç¡®çš„å¹²æ”¯-å…¬å†è½¬æ¢
2. æœˆåº¦æ•°æ®æ›´ç¨³å®šï¼Œå‡å°‘å™ªéŸ³
3. è¶³å¤Ÿå±•ç¤ºé•¿æœŸè¶‹åŠ¿
4. å¯ä»¥å‡†ç¡®å®šä½å…³é”®æœˆä»½ï¼ˆå¦‚å˜‰é–21å¹´10æœˆå£¬å¯…å®«å˜ï¼‰
"""
import sys
import re
from collections import defaultdict
from pathlib import Path
import json

if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')


class MonthlyAnalyzer:
    """æœˆåº¦æ—¶é—´åºåˆ—åˆ†æå™¨"""

    def __init__(self):
        # æ ¸å¿ƒå˜é‡ï¼šé‡é‡‘å±æ‘„å…¥ï¼ˆXè½´ï¼‰
        self.toxicity_keywords = {
            # ç›´æ¥æ‘„å…¥ - é«˜æƒé‡
            'è¿›ä¸¹': 10, 'èµè¯': 10, 'æœé£Ÿ': 10,
            'çº¢é“…': 10, 'ç§‹çŸ³': 10, 'ç”˜éœ²': 6,

            # ç‚¼ä¸¹äººç‰©
            'é™¶ä»²æ–‡': 8, 'é‚µå…ƒèŠ‚': 8, 'ç‹é‡‘': 6,
            'æ®µæœç”¨': 6, 'é¡¾å¯å­¦': 6,

            # é“æ•™ä»ªå¼ï¼ˆé—´æ¥æŒ‡æ ‡ï¼‰
            'é†®': 5, 'ç¥·': 5, 'ç¥€': 3, 'æ–‹': 3,
            'ç¬¦': 4, 'æ³•': 2,

            # èº«ä½“ç—‡çŠ¶ï¼ˆä¸­æ¯’è¡¨ç°ï¼‰
            'ä¸è±«': 4, 'å¿ƒæ‚¸': 6, 'çœ©æ™•': 5,
            'éœ‡é¢¤': 7, 'æš´èº': 5, 'ç–¾': 2
        }

        # æ ¸å¿ƒå˜é‡ï¼šæ”¿æ²»æš´è™ï¼ˆYè½´ï¼‰
        self.tyranny_keywords = {
            # æç«¯æš´åŠ› - æœ€é«˜æƒé‡
            'å‡Œè¿Ÿ': 10, 'ç£”': 10, 'é”‰å°¸': 10,
            'æ­ç¤º': 8, 'æ–©': 7,

            # å»·æ–ï¼ˆå®«å»·ä½“ç½šï¼‰
            'å»·æ–': 10, 'æ–': 5,

            # å¼ºåˆ¶é€€ä¼‘/é™çº§
            'è‡´ä»•': 2, 'å‰ŠèŒ': 5, 'å¤ºå®˜': 5,
            'ç½¢': 3, 'é»œ': 4,

            # æƒ…ç»ªçˆ†å‘
            'éœ‡æ€’': 6, 'å¤§æ€’': 6, 'æ€’': 3,
            'æ–¥': 4, 'è´£': 2,

            # å®«å˜ç›¸å…³
            'é€†': 3, 'è°‹': 2, 'å®«å˜': 10,
            'å¼‘': 10, 'ç¼¢': 8
        }

        # æ§åˆ¶å˜é‡ï¼šå¤–éƒ¨å‹åŠ›å› ç´ 
        self.control_keywords = {
            # è¾¹é˜²å‹åŠ›
            'åŒ—è™': 5, 'è’™å¤': 5, 'å€­å¯‡': 5,
            'è¾¹æ‚£': 4, 'å¯‡': 2, 'å…µ': 1,

            # è‡ªç„¶ç¾å®³
            'åœ°éœ‡': 6, 'æ´ªæ°´': 5, 'æ—±': 4,
            'ç¾': 3, 'é¥¥': 4, 'ç–«': 5,

            # å†…éƒ¨åå¯¹
            'è°': 3, 'äº‰': 3, 'åŠ': 2
        }

    def parse_text_by_month(self, text_file):
        """
        æŒ‰æœˆä»½è§£ææ–‡æœ¬

        è¿”å›:
            Dict: {
                (year, month): {
                    "text": "è¯¥æœˆçš„æ‰€æœ‰æ–‡æœ¬",
                    "char_count": å­—ç¬¦æ•°
                }
            }
        """
        with open(text_file, 'r', encoding='utf-8') as f:
            content = f.read()

        monthly_data = defaultdict(lambda: {'text': '', 'char_count': 0})

        # æ­£åˆ™ï¼šå˜‰é–Xå¹´Xæœˆ
        year_pattern = re.compile(r'å˜‰é–([å…ƒä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)å¹´')
        month_pattern = re.compile(r'([æ­£äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå†¬è…Š][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]?)æœˆ')

        # ä¸­æ–‡æ•°å­—æ˜ å°„
        cn_num_map = {
            'ã€‡': 0, 'é›¶': 0, 'å…ƒ': 1, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3,
            'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
            'å': 10, 'æ­£': 1, 'å†¬': 11, 'è…Š': 12
        }

        def cn_to_num(cn_str):
            """ä¸­æ–‡æ•°å­—è½¬æ•°å­—"""
            if cn_str in cn_num_map:
                return cn_num_map[cn_str]
            if 'å' in cn_str:
                parts = cn_str.split('å')
                left = cn_num_map.get(parts[0], 0) if parts[0] else 1
                right = cn_num_map.get(parts[1], 0) if parts[1] else 0
                return left * 10 + right
            return 0

        # æŒ‰è¡Œæ‰«æ
        current_year = None
        current_month = None

        lines = content.split('\n')

        for line in lines:
            # æ£€æµ‹å¹´ä»½
            year_match = year_pattern.search(line)
            if year_match:
                current_year = cn_to_num(year_match.group(1))

            # æ£€æµ‹æœˆä»½
            month_match = month_pattern.search(line)
            if month_match:
                current_month = cn_to_num(month_match.group(1))

            # å¦‚æœæœ‰å®Œæ•´çš„å¹´æœˆä¿¡æ¯ï¼Œè®°å½•è¯¥è¡Œ
            if current_year and current_month:
                key = (current_year, current_month)
                monthly_data[key]['text'] += line + '\n'
                monthly_data[key]['char_count'] += len(line)

        return dict(monthly_data)

    def calculate_monthly_scores(self, monthly_data):
        """
        è®¡ç®—æ¯æœˆçš„æ¯’æ€§ã€æš´è™ã€æ§åˆ¶å˜é‡åˆ†æ•°

        è¿”å›:
            List[Dict]: æŒ‰æ—¶é—´æ’åºçš„æœˆåº¦æ•°æ®
        """
        results = []

        for (year, month), data in sorted(monthly_data.items()):
            text = data['text']

            # è®¡ç®—ä¸‰ç±»æŒ‡æ ‡
            toxicity_score = sum(
                weight * text.count(keyword)
                for keyword, weight in self.toxicity_keywords.items()
            )

            tyranny_score = sum(
                weight * text.count(keyword)
                for keyword, weight in self.tyranny_keywords.items()
            )

            control_score = sum(
                weight * text.count(keyword)
                for keyword, weight in self.control_keywords.items()
            )

            # æ ‡å‡†åŒ–ï¼ˆæŒ‰å­—ç¬¦æ•°ï¼‰
            char_count = data['char_count']
            if char_count > 0:
                toxicity_normalized = (toxicity_score / char_count) * 10000
                tyranny_normalized = (tyranny_score / char_count) * 10000
                control_normalized = (control_score / char_count) * 10000
            else:
                toxicity_normalized = 0
                tyranny_normalized = 0
                control_normalized = 0

            results.append({
                'year': year,
                'month': month,
                'toxicity_raw': toxicity_score,
                'tyranny_raw': tyranny_score,
                'control_raw': control_score,
                'toxicity_norm': round(toxicity_normalized, 2),
                'tyranny_norm': round(tyranny_normalized, 2),
                'control_norm': round(control_normalized, 2),
                'char_count': char_count
            })

        return results

    def analyze_file(self, text_file, output_dir="analysis_results"):
        """å®Œæ•´åˆ†ææµç¨‹"""

        print("="*60)
        print(f"æœˆåº¦æ—¶é—´åºåˆ—åˆ†æ: {Path(text_file).name}")
        print("="*60)

        # 1. æŒ‰æœˆè§£æ
        monthly_data = self.parse_text_by_month(text_file)
        print(f"\nâœ“ è§£æå®Œæˆï¼Œå…± {len(monthly_data)} ä¸ªæœˆä»½çš„æ•°æ®")

        if not monthly_data:
            print("âœ— æ— æ•°æ®")
            return None

        # è·å–æ—¶é—´èŒƒå›´
        years = [y for y, m in monthly_data.keys()]
        print(f"âœ“ æ—¶é—´èŒƒå›´: å˜‰é–{min(years)}å¹´ - å˜‰é–{max(years)}å¹´")

        # 2. è®¡ç®—æœˆåº¦æŒ‡æ ‡
        monthly_scores = self.calculate_monthly_scores(monthly_data)

        # 3. è¾“å‡ºç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        result_file = output_path / "monthly_timeseries.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(monthly_scores, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ ç»“æœä¿å­˜: {result_file}")

        # 4. æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        print(f"\n{'='*60}")
        print("ç»Ÿè®¡æ‘˜è¦")
        print("="*60)

        avg_toxicity = sum(m['toxicity_norm'] for m in monthly_scores) / len(monthly_scores)
        avg_tyranny = sum(m['tyranny_norm'] for m in monthly_scores) / len(monthly_scores)

        print(f"å¹³å‡æ¯’æ€§æŒ‡æ•°ï¼ˆæ ‡å‡†åŒ–ï¼‰: {avg_toxicity:.2f}")
        print(f"å¹³å‡æš´è™æŒ‡æ•°ï¼ˆæ ‡å‡†åŒ–ï¼‰: {avg_tyranny:.2f}")

        # æ‰¾åˆ°å³°å€¼æœˆä»½
        max_toxicity_month = max(monthly_scores, key=lambda x: x['toxicity_norm'])
        max_tyranny_month = max(monthly_scores, key=lambda x: x['tyranny_norm'])

        print(f"\næ¯’æ€§å³°å€¼: å˜‰é–{max_toxicity_month['year']}å¹´{max_toxicity_month['month']}æœˆ ({max_toxicity_month['toxicity_norm']:.2f})")
        print(f"æš´è™å³°å€¼: å˜‰é–{max_tyranny_month['year']}å¹´{max_tyranny_month['month']}æœˆ ({max_tyranny_month['tyranny_norm']:.2f})")

        # 5. ç‰¹åˆ«å…³æ³¨å£¬å¯…å®«å˜æœˆä»½ï¼ˆå˜‰é–21å¹´10æœˆï¼‰
        gongbian_month = next((m for m in monthly_scores if m['year'] == 21 and m['month'] == 10), None)

        if gongbian_month:
            print(f"\n{'='*60}")
            print("ğŸ¯ å£¬å¯…å®«å˜æœˆä»½ï¼ˆå˜‰é–21å¹´10æœˆï¼‰ä¸“é¡¹åˆ†æ")
            print("="*60)
            print(f"æ¯’æ€§æŒ‡æ•°: {gongbian_month['toxicity_norm']:.2f}")
            print(f"æš´è™æŒ‡æ•°: {gongbian_month['tyranny_norm']:.2f}")
            print(f"å¤–éƒ¨å‹åŠ›: {gongbian_month['control_norm']:.2f}")
            print(f"æ–‡æœ¬å­—æ•°: {gongbian_month['char_count']:,}")

        return monthly_scores


if __name__ == "__main__":
    analyzer = MonthlyAnalyzer()

    # åˆ†ææ—©æœŸæ•°æ®ï¼ˆåŸºçº¿ï¼‰
    early_file = Path("jiajing_data_from_pdf/complete_vol1-45.txt")
    if early_file.exists():
        print("\nğŸ“Š åŸºçº¿æœŸåˆ†æï¼ˆå˜‰é–æ—©æœŸï¼‰\n")
        early_results = analyzer.analyze_file(
            early_file,
            output_dir="analysis_results/monthly_early"
        )

    # åˆ†æå£¬å¯…æ—¶æœŸæ•°æ®
    renyin_file = Path("jiajing_data_from_pdf/renyin_gongbian_era_vol228-276.txt")
    if renyin_file.exists():
        print("\n\nğŸ“Š å£¬å¯…æ—¶æœŸåˆ†æï¼ˆå˜‰é–19-23å¹´ï¼‰\n")
        renyin_results = analyzer.analyze_file(
            renyin_file,
            output_dir="analysis_results/monthly_renyin"
        )
