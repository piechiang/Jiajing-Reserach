"""
æ–‡æœ¬åˆ†æå·¥å…· - å¯¹ä¸‹è½½çš„å˜‰é–å®å½•è¿›è¡ŒåŸºç¡€åˆ†æ
"""
import re
from pathlib import Path
from collections import Counter
import json


class JiajingTextAnalyzer:
    """å˜‰é–å®å½•æ–‡æœ¬åˆ†æå™¨"""

    def __init__(self, data_dir="jiajing_data"):
        self.data_dir = Path(data_dir)

    def load_volume(self, volume_num):
        """åŠ è½½æŒ‡å®šå·çš„æ–‡æœ¬"""
        filepath = self.data_dir / f"jiajing_shilu_vol{volume_num}.txt"

        if not filepath.exists():
            return None

        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()

    def load_all_volumes(self):
        """åŠ è½½æ‰€æœ‰å·²ä¸‹è½½çš„å·"""
        all_text = []
        # åªåŠ è½½å•å·æ–‡ä»¶ï¼Œæ’é™¤åˆå¹¶æ–‡ä»¶
        volumes = []
        for f in self.data_dir.glob('jiajing_shilu_vol*.txt'):
            # è·³è¿‡åˆå¹¶æ–‡ä»¶ (åŒ…å«"-"çš„æ–‡ä»¶å)
            if '-' in f.stem or 'complete' in f.stem:
                continue
            try:
                vol_num = int(f.stem.replace('jiajing_shilu_vol', ''))
                volumes.append(vol_num)
            except ValueError:
                continue
        volumes = sorted(volumes)

        for vol in volumes:
            text = self.load_volume(vol)
            if text:
                all_text.append((vol, text))

        return all_text

    def basic_stats(self, text):
        """åŸºç¡€ç»Ÿè®¡"""
        # ç§»é™¤æ ‡é¢˜å’Œåˆ†éš”çº¿
        lines = text.split('\n')
        content = '\n'.join(lines[2:]) if len(lines) > 2 else text

        stats = {
            'æ€»å­—æ•°': len(content),
            'æ€»è¡Œæ•°': len(content.split('\n')),
            'æ®µè½æ•°': len([p for p in content.split('\n') if p.strip()]),
        }

        return stats

    def extract_person_names(self, text, name_list=None):
        """
        æå–äººç‰©åå­—å‡ºç°æ¬¡æ•°

        Args:
            text: æ–‡æœ¬å†…å®¹
            name_list: äººç‰©åå•åˆ—è¡¨ï¼Œå¦‚ ['æ¨å»·å’Œ', 'å¼ ç’', 'æ¡‚è¼']

        Returns:
            Counterå¯¹è±¡ï¼ŒåŒ…å«æ¯ä¸ªäººåçš„å‡ºç°æ¬¡æ•°
        """
        if name_list is None:
            # é»˜è®¤çš„å¤§ç¤¼è®®æ ¸å¿ƒäººç‰©
            name_list = [
                'æ¨å»·å’Œ', 'è’‹å†•', 'æ¯›æ¾„',  # åå¯¹æ´¾
                'å¼ ç’', 'æ¡‚è¼', 'æ–¹çŒ®å¤«',  # æ”¯æŒæ´¾
                'è´¹å®', 'æ¨ä¸€æ¸…',  # ä¸­é—´æ´¾
                'ä¸–å®—', 'æ˜ä¸–å®—', 'å˜‰é–',  # çš‡å¸
                'å…´çŒ®ç‹', 'çŒ®çš‡å¸'  # å˜‰é–ç”Ÿçˆ¶
            ]

        name_counts = Counter()

        for name in name_list:
            count = text.count(name)
            if count > 0:
                name_counts[name] = count

        return name_counts

    def find_person_contexts(self, text, person_name, context_length=50):
        """
        æŸ¥æ‰¾äººç‰©åå­—å‡ºç°çš„ä¸Šä¸‹æ–‡

        Args:
            text: æ–‡æœ¬å†…å®¹
            person_name: äººç‰©åå­—
            context_length: ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰

        Returns:
            list: åŒ…å«è¯¥äººç‰©çš„æ–‡æœ¬ç‰‡æ®µ
        """
        contexts = []
        pos = 0

        while True:
            pos = text.find(person_name, pos)
            if pos == -1:
                break

            start = max(0, pos - context_length)
            end = min(len(text), pos + len(person_name) + context_length)

            context = text[start:end]
            contexts.append({
                'position': pos,
                'context': context,
                'before': text[start:pos],
                'name': person_name,
                'after': text[pos + len(person_name):end]
            })

            pos += len(person_name)

        return contexts

    def extract_dates(self, text):
        """æå–æ—¥æœŸä¿¡æ¯"""
        # åŒ¹é…ç±»ä¼¼"å˜‰é–å…ƒå¹´æ­£æœˆç”²å­"çš„æ—¥æœŸ
        date_pattern = r'(å˜‰é–\d+å¹´.*?[å¹´æœˆæ—¥])'
        dates = re.findall(date_pattern, text)

        return Counter(dates[:20])  # è¿”å›å‰20ä¸ªæ—¥æœŸ

    def keyword_frequency(self, text, top_n=50):
        """
        å…³é”®è¯é¢‘ç‡åˆ†æ

        Args:
            text: æ–‡æœ¬å†…å®¹
            top_n: è¿”å›å‰Nä¸ªé«˜é¢‘è¯

        Returns:
            Counterå¯¹è±¡
        """
        # ç®€å•çš„å•å­—å’ŒåŒå­—è¯æå–
        # æ³¨æ„: è¿™æ˜¯ç®€åŒ–ç‰ˆï¼ŒçœŸå®åº”ç”¨åº”ä½¿ç”¨jiebaç­‰åˆ†è¯å·¥å…·

        # è¿‡æ»¤å¸¸ç”¨è™šè¯
        stop_words = set('ä¹‹ä¹è€…ä¹Ÿã€ã€‚ï¼Œçš„äº†æ˜¯åœ¨æœ‰ä¸ºè€Œæ–¼ä»¥èˆ‡å…¶å‰‡æ›°ä»¥åŠ')

        # æå–2-4å­—çš„è¯
        words = []
        for length in [2, 3, 4]:
            for i in range(len(text) - length + 1):
                word = text[i:i+length]
                if not any(c in stop_words for c in word):
                    words.append(word)

        return Counter(words).most_common(top_n)

    def analyze_volume(self, volume_num):
        """åˆ†æå•å·"""
        text = self.load_volume(volume_num)

        if not text:
            print(f"âŒ æœªæ‰¾åˆ°å·{volume_num}")
            return None

        print(f"\n{'='*60}")
        print(f"å·{volume_num} åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}\n")

        # åŸºç¡€ç»Ÿè®¡
        stats = self.basic_stats(text)
        print("ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value:,}")

        # äººç‰©æåŠ
        print("\nğŸ‘¥ æ ¸å¿ƒäººç‰©æåŠæ¬¡æ•°:")
        person_counts = self.extract_person_names(text)
        for name, count in person_counts.most_common(10):
            print(f"  {name}: {count}æ¬¡")

        # é«˜é¢‘è¯
        print("\nğŸ”¤ é«˜é¢‘è¯æ±‡ (Top 20):")
        keywords = self.keyword_frequency(text, top_n=20)
        for word, count in keywords:
            print(f"  {word}: {count}æ¬¡")

        return {
            'stats': stats,
            'persons': dict(person_counts),
            'keywords': dict(keywords)
        }

    def analyze_all(self):
        """åˆ†ææ‰€æœ‰å·²ä¸‹è½½çš„å·"""
        volumes = self.load_all_volumes()

        if not volumes:
            print("âŒ æœªæ‰¾åˆ°å·²ä¸‹è½½çš„æ•°æ®")
            return

        print(f"\n{'='*60}")
        print(f"å…¨æ–‡åˆ†ææŠ¥å‘Š (å…±{len(volumes)}å·)")
        print(f"{'='*60}\n")

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = '\n'.join([text for _, text in volumes])

        # æ€»ä½“ç»Ÿè®¡
        stats = self.basic_stats(all_text)
        print("ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value:,}")

        # äººç‰©ç»Ÿè®¡
        print("\nğŸ‘¥ æ ¸å¿ƒäººç‰©æ€»æåŠæ¬¡æ•°:")
        person_counts = self.extract_person_names(all_text)
        for name, count in person_counts.most_common(15):
            print(f"  {name}: {count}æ¬¡")

        # æŒ‰å·ç»Ÿè®¡äººç‰©å‡ºç°
        print("\nğŸ“ˆ äººç‰©å‡ºç°åˆ†å¸ƒ (æŒ‰å·):")
        top_persons = [name for name, _ in person_counts.most_common(5)]

        for person in top_persons:
            print(f"\n  {person}:")
            for vol, text in volumes[:10]:  # åªæ˜¾ç¤ºå‰10å·
                count = text.count(person)
                if count > 0:
                    bar = 'â–ˆ' * min(count, 50)
                    print(f"    å·{vol:2d}: {bar} ({count})")

        # ä¿å­˜æŠ¥å‘Š
        report = {
            'total_volumes': len(volumes),
            'volume_range': [volumes[0][0], volumes[-1][0]],
            'stats': stats,
            'persons': dict(person_counts),
            'keywords': dict(self.keyword_frequency(all_text, top_n=50))
        }

        report_file = self.data_dir / 'analysis_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def search_keyword(self, keyword, max_results=10):
        """æœç´¢å…³é”®è¯åœ¨æ‰€æœ‰å·ä¸­çš„å‡ºç°"""
        volumes = self.load_all_volumes()

        print(f"\næœç´¢å…³é”®è¯: '{keyword}'")
        print("=" * 60)

        results = []

        for vol, text in volumes:
            contexts = self.find_person_contexts(text, keyword, context_length=40)

            if contexts:
                print(f"\nğŸ“– å·{vol} (å…±{len(contexts)}å¤„):")

                for i, ctx in enumerate(contexts[:max_results]):
                    print(f"\n  [{i+1}] ...{ctx['context']}...")

                results.append((vol, contexts))

        print(f"\nâœ“ å…±åœ¨{len(results)}å·ä¸­æ‰¾åˆ°'{keyword}'")
        return results


def main():
    """ä¸»ç¨‹åº"""
    print("å˜‰é–å®å½•æ–‡æœ¬åˆ†æå·¥å…·")
    print("=" * 60)

    analyzer = JiajingTextAnalyzer(data_dir="jiajing_data")

    print("\né€‰æ‹©åŠŸèƒ½:")
    print("1. åˆ†æå•å·")
    print("2. åˆ†ææ‰€æœ‰å·²ä¸‹è½½çš„å·")
    print("3. æœç´¢å…³é”®è¯")
    print("4. åˆ†æç‰¹å®šäººç‰©")
    print("=" * 60)

    choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()

    if choice == "1":
        vol = int(input("è¯·è¾“å…¥å·å·: "))
        analyzer.analyze_volume(vol)

    elif choice == "2":
        analyzer.analyze_all()

    elif choice == "3":
        keyword = input("è¯·è¾“å…¥å…³é”®è¯: ")
        analyzer.search_keyword(keyword, max_results=5)

    elif choice == "4":
        person = input("è¯·è¾“å…¥äººç‰©åå­—: ")
        volumes = analyzer.load_all_volumes()
        all_text = '\n'.join([text for _, text in volumes])

        print(f"\nåˆ†æäººç‰©: {person}")
        print("=" * 60)

        # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
        count = all_text.count(person)
        print(f"æ€»æåŠæ¬¡æ•°: {count}")

        # æŸ¥æ‰¾ä¸Šä¸‹æ–‡
        contexts = analyzer.find_person_contexts(all_text, person, context_length=60)
        print(f"\nå‰5ä¸ªå‡ºç°ä¸Šä¸‹æ–‡:")
        for i, ctx in enumerate(contexts[:5], 1):
            print(f"\n[{i}] ...{ctx['context']}...")

    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    main()
