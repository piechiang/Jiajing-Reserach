# -*- coding: utf-8 -*-
"""
å£¬å¯…å®«å˜ä¸“é¡¹åˆ†æ
éªŒè¯"ç‚¼ä¸¹â†’é‡é‡‘å±ä¸­æ¯’â†’æš´è™â†’å®«å˜"çš„å› æœé“¾
"""
import sys
import re
from pathlib import Path
from collections import defaultdict
import json

if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')


class RenyinAnalyzer:
    """å£¬å¯…å®«å˜ä¸“é¡¹åˆ†æå™¨"""

    def __init__(self, data_file):
        self.data_file = Path(data_file)
        self.content = ""
        self.load_data()

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        if not self.data_file.exists():
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {self.data_file}")
            return False

        with open(self.data_file, 'r', encoding='utf-8') as f:
            self.content = f.read()

        print(f"âœ“ å·²åŠ è½½æ•°æ®: {len(self.content):,}å­—")
        return True

    def search_palace_incident_keywords(self):
        """æœç´¢å®«å˜ç›¸å…³çš„æ‰€æœ‰å¯èƒ½å…³é”®è¯"""
        print("\n" + "="*60)
        print("ç¬¬ä¸€æ­¥ï¼šæœç´¢å®«å˜äº‹ä»¶å…³é”®è¯")
        print("="*60)

        # å®«å˜å¯èƒ½çš„å„ç§è¯´æ³•
        keywords = {
            # ç›´æ¥è¯´æ³•
            'å®«å˜': 0, 'æ¨é‡‘è‹±': 0, 'å¼‘': 0, 'å¼‘å›': 0,

            # å§”å©‰è¯´æ³•
            'å®«äºº': 0, 'é€†è°‹': 0, 'å†…å˜': 0, 'å®«é—±': 0,
            'å¦–å¦‡': 0, 'å¦–äºº': 0, 'å›é€†': 0,

            # ç›¸å…³äººç‰©
            'æ–¹çš‡å': 0, 'æ›¹ç«¯å¦ƒ': 0, 'ç‹å®å«”': 0,

            # åœ°ç‚¹
            'ç«¯æœ¬å®«': 0, 'æ°¸å¯¿å®«': 0, 'é•¿æ˜¥å®«': 0,

            # äº‹ä»¶çº¿ç´¢
            'å‹’æ­»': 0, 'ç¼¢': 0, 'è°‹å®³': 0, 'é€†å®«äºº': 0
        }

        for keyword in keywords.keys():
            keywords[keyword] = self.content.count(keyword)

        print("\nå…³é”®è¯ç»Ÿè®¡:")
        found_any = False
        for kw, count in sorted(keywords.items(), key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"  âœ“ '{kw}': {count}æ¬¡")
                found_any = True

        if not found_any:
            print("  âš ï¸ æœªæ‰¾åˆ°ç›´æ¥çš„å®«å˜è®°å½•")
            print("  è¯´æ˜: å²ä¹¦å¯èƒ½é¿è®³ï¼Œæˆ–æˆ‘ä»¬æå–çš„é¡µç èŒƒå›´ä¸å‡†ç¡®")

        return keywords

    def extract_toxicity_enhanced(self):
        """
        å¢å¼ºç‰ˆæ¯’æ€§æŒ‡æ ‡æå–ï¼ˆé’ˆå¯¹å˜‰é–ä¸­æœŸï¼‰
        è¿™ä¸ªæ—¶æœŸç‚¼ä¸¹æ´»åŠ¨åº”è¯¥æ›´é¢‘ç¹
        """
        print("\n" + "="*60)
        print("ç¬¬äºŒæ­¥ï¼šæå–é‡é‡‘å±æ‘„å…¥/ä¿®é“æ´»åŠ¨æŒ‡æ ‡ï¼ˆå¢å¼ºç‰ˆï¼‰")
        print("="*60)

        # æ‰©å±•å…³é”®è¯åº“
        keywords = {
            # ç›´æ¥ä»£ç†ï¼ˆé«˜æƒé‡ï¼‰
            'è¿›ä¸¹': 10, 'èµè¯': 10, 'çº¢é“…': 10, 'ç§‹çŸ³': 10,
            'é‡‘ä¸¹': 10, 'çµè¯': 8, 'ä»™è¯': 8,

            # é“å£«åå­—ï¼ˆå˜‰é–ä¸­æœŸé‡è¦é“å£«ï¼‰
            'é™¶ä»²æ–‡': 10, 'é‚µå…ƒèŠ‚': 10, 'æ®µæœç”¨': 8,

            # é—´æ¥ä»£ç†ï¼ˆä¿®é“æ´»åŠ¨ï¼‰
            'é†®': 5, 'ç¥·': 5, 'ç¥€': 3, 'æ–‹': 3,
            'å»ºé†®': 7, 'ç¥·ç¥€': 6, 'é›·å›': 6,
            'ä¿®æ–‹': 5, 'æ–‹æˆ’': 4,

            # å®«æ®¿ï¼ˆä¸“é—¨ä¿®é“çš„åœ°æ–¹ï¼‰
            'ç„æå®æ®¿': 8, 'é’¦å®‰æ®¿': 6,

            # ç—…ç†ååº”ï¼ˆé‡é‡‘å±ä¸­æ¯’ç—‡çŠ¶ï¼‰
            'ä¸è±«': 5, 'ç”šè‡³': 2, 'å¿ƒæ‚¸': 7,
            'ä¸èƒ½è§†æœ': 6, 'å¯ç–¾': 5, 'èº': 6,

            # å®«å¥³ç›¸å…³ï¼ˆç‚¼ä¸¹éœ€è¦å®«å¥³é‡‡é›†ç”˜éœ²ï¼‰
            'é‡‡éœ²': 8, 'ç”˜éœ²': 6, 'éœ²æ°´': 5
        }

        toxicity_events = []

        for keyword, weight in keywords.items():
            pos = 0
            count = 0

            while True:
                pos = self.content.find(keyword, pos)
                if pos == -1:
                    break

                # æå–ä¸Šä¸‹æ–‡
                start = max(0, pos - 300)
                end = min(len(self.content), pos + len(keyword) + 300)
                context = self.content[start:end]

                # æå–æ—¥æœŸ
                date_match = re.search(r'(å˜‰é–\w+å¹´\w+æœˆ\w+)', context)
                date = date_match.group(1) if date_match else "æœªçŸ¥æ—¥æœŸ"

                toxicity_events.append({
                    'position': pos,
                    'date': date,
                    'keyword': keyword,
                    'weight': weight,
                    'context': context
                })

                count += 1
                pos += len(keyword)

            if count > 0:
                print(f"  [{keyword}]: {count}æ¬¡ (æƒé‡={weight})")

        # æŒ‰ä½ç½®æ’åº
        toxicity_events.sort(key=lambda x: x['position'])

        print(f"\nâœ“ å…±æ‰¾åˆ° {len(toxicity_events)} ä¸ªé‡é‡‘å±/ä¿®é“æ´»åŠ¨æŒ‡æ ‡")
        total_weight = sum(e['weight'] for e in toxicity_events)
        print(f"âœ“ ç´¯ç§¯æ¯’æ€§æƒé‡: {total_weight}")

        return toxicity_events

    def extract_tyranny_enhanced(self):
        """å¢å¼ºç‰ˆæš´è™æŒ‡æ ‡æå–"""
        print("\n" + "="*60)
        print("ç¬¬ä¸‰æ­¥ï¼šæå–æ”¿æ²»æš´è™æŒ‡æ ‡ï¼ˆå¢å¼ºç‰ˆï¼‰")
        print("="*60)

        keywords = {
            # ä¸€çº§æš´è™ï¼ˆè‡´æ­»/è‚‰ä½“ä¼¤å®³ï¼‰
            'å»·æ–': 10, 'æ¯™äºæ–ä¸‹': 10, 'å¼ƒå¸‚': 10,
            'æ–©': 10, 'ç»': 10, 'å‡Œè¿Ÿ': 10,
            'ä¸‹è¯ç‹±': 9, 'é”¦è¡£å«': 7,

            # äºŒçº§æš´è™ï¼ˆæ”¿æ²»æ¸…æ´—ï¼‰
            'å‰Šç±': 5, 'ä¸ºæ°‘': 4, 'è‡´ä»•': 2,
            'è¤«å¤º': 6, 'åˆ‡è´£': 5, 'ç½¢é»œ': 4,
            'è´¬è°ª': 5, 'æˆè¾¹': 6, 'å……å†›': 6,

            # ä¸‰çº§æš´è™ï¼ˆæƒ…ç»ªå®£æ³„ï¼‰
            'éœ‡æ€’': 4, 'å¤§æ€’': 5, 'æ·è¡¨': 3,
            'å±é€€': 2, 'è´£éª‚': 3,

            # é’ˆå¯¹å®«å¥³çš„æš´è™ï¼ˆå£¬å¯…å®«å˜ç‰¹æœ‰ï¼‰
            'æ–å®«äºº': 10, 'è´£å®«äºº': 7, 'ç½šå®«äºº': 5
        }

        tyranny_events = []

        for keyword, score in keywords.items():
            pos = 0
            count = 0

            while True:
                pos = self.content.find(keyword, pos)
                if pos == -1:
                    break

                # æå–ä¸Šä¸‹æ–‡
                start = max(0, pos - 300)
                end = min(len(self.content), pos + len(keyword) + 300)
                context = self.content[start:end]

                # æå–æ—¥æœŸ
                date_match = re.search(r'(å˜‰é–\w+å¹´\w+æœˆ\w+)', context)
                date = date_match.group(1) if date_match else "æœªçŸ¥æ—¥æœŸ"

                tyranny_events.append({
                    'position': pos,
                    'date': date,
                    'keyword': keyword,
                    'score': score,
                    'context': context
                })

                count += 1
                pos += len(keyword)

            if count > 0:
                print(f"  [{keyword}]: {count}æ¬¡ (åˆ†æ•°={score})")

        tyranny_events.sort(key=lambda x: x['position'])

        print(f"\nâœ“ å…±æ‰¾åˆ° {len(tyranny_events)} ä¸ªæš´è™äº‹ä»¶æŒ‡æ ‡")
        total_score = sum(e['score'] for e in tyranny_events)
        print(f"âœ“ ç´¯ç§¯æš´è™åˆ†æ•°: {total_score}")

        return tyranny_events

    def compare_with_early_jiajing(self):
        """ä¸å˜‰é–åˆæœŸæ•°æ®å¯¹æ¯”"""
        print("\n" + "="*60)
        print("ç¬¬å››æ­¥ï¼šä¸å˜‰é–åˆæœŸå¯¹æ¯”åˆ†æ")
        print("="*60)

        # å˜‰é–åˆæœŸæ•°æ®ï¼ˆæ¥è‡ªä¹‹å‰çš„åˆ†æï¼‰
        early_data = {
            'chars': 683965,
            'toxicity_events': 84,
            'toxicity_weight': 305,
            'tyranny_events': 540,
            'tyranny_score': 3041
        }

        # å½“å‰æ•°æ®
        current_chars = len(self.content)

        # é‡æ–°æå–ç”¨äºå¯¹æ¯”
        tox = self.extract_toxicity_enhanced()
        tyr = self.extract_tyranny_enhanced()

        current_data = {
            'chars': current_chars,
            'toxicity_events': len(tox),
            'toxicity_weight': sum(e['weight'] for e in tox),
            'tyranny_events': len(tyr),
            'tyranny_score': sum(e['score'] for e in tyr)
        }

        print("\nå¯¹æ¯”ç»“æœ:\n")
        print(f"{'æŒ‡æ ‡':<20} {'å˜‰é–åˆæœŸ(1-3å¹´)':<20} {'å£¬å¯…æ—¶æœŸ(19-23å¹´)':<20} {'å˜åŒ–ç‡':<15}")
        print("-" * 80)

        # å½’ä¸€åŒ–åˆ°æ¯10ä¸‡å­—
        normalize = 100000

        for key in ['toxicity_events', 'toxicity_weight', 'tyranny_events', 'tyranny_score']:
            early_val = early_data[key] / early_data['chars'] * normalize
            current_val = current_data[key] / current_data['chars'] * normalize

            if early_val > 0:
                change = ((current_val - early_val) / early_val) * 100
                change_str = f"+{change:.1f}%" if change > 0 else f"{change:.1f}%"
            else:
                change_str = "N/A"

            label = {
                'toxicity_events': 'ä¿®é“æ´»åŠ¨é¢‘æ¬¡',
                'toxicity_weight': 'ç´¯ç§¯æ¯’æ€§æƒé‡',
                'tyranny_events': 'æš´è™äº‹ä»¶é¢‘æ¬¡',
                'tyranny_score': 'ç´¯ç§¯æš´è™åˆ†æ•°'
            }[key]

            print(f"{label:<20} {early_val:<20.1f} {current_val:<20.1f} {change_str:<15}")

        return early_data, current_data

    def generate_renyin_report(self, palace_keywords, tox_events, tyr_events, comparison):
        """ç”Ÿæˆå£¬å¯…å®«å˜ä¸“é¡¹æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ç¬¬äº”æ­¥ï¼šç”Ÿæˆå£¬å¯…å®«å˜åˆ†ææŠ¥å‘Š")
        print("="*60)

        report_file = Path("å£¬å¯…å®«å˜æ—¶æœŸæ¯’æ€§æš´è™åˆ†æ.md")

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# å£¬å¯…å®«å˜æ—¶æœŸé‡é‡‘å±ä¸­æ¯’ä¸æ”¿æ²»æš´è™åˆ†ææŠ¥å‘Š\n\n")

            f.write("## ç ”ç©¶ç›®æ ‡\n\n")
            f.write("éªŒè¯å˜‰é–å¸åœ¨å£¬å¯…å®«å˜(1542å¹´)å‰åçš„ç‚¼ä¸¹æ´»åŠ¨ä¸æš´è™è¡Œä¸ºæ˜¯å¦å­˜åœ¨ç›¸å…³æ€§\n\n")

            f.write("## æ•°æ®æ¥æº\n\n")
            f.write(f"- æ–‡ä»¶: {self.data_file}\n")
            f.write(f"- å­—æ•°: {len(self.content):,}å­—\n")
            f.write(f"- è¦†ç›–: å˜‰é–19-23å¹´ (1540-1544)\n")
            f.write(f"- PDFé¡µç : 2500-3040é¡µ\n\n")

            f.write("## å®«å˜äº‹ä»¶å…³é”®è¯æ£€ç´¢\n\n")
            found_keywords = [(k, v) for k, v in palace_keywords.items() if v > 0]
            if found_keywords:
                f.write("| å…³é”®è¯ | å‡ºç°æ¬¡æ•° |\n")
                f.write("|--------|----------|\n")
                for kw, count in sorted(found_keywords, key=lambda x: x[1], reverse=True):
                    f.write(f"| {kw} | {count} |\n")
            else:
                f.write("âš ï¸ **æœªæ‰¾åˆ°ç›´æ¥çš„å®«å˜è®°å½•**\n\n")
                f.write("å¯èƒ½åŸå› :\n")
                f.write("1. å²ä¹¦é¿è®³ï¼Œä½¿ç”¨å§”å©‰è¯´æ³•\n")
                f.write("2. æå–çš„é¡µç èŒƒå›´éœ€è¦è°ƒæ•´\n")
                f.write("3. å£¬å¯…å®«å˜è®°å½•åœ¨å…¶ä»–å·å†Œ\n\n")

            f.write("## Xè½´ï¼šé‡é‡‘å±æ‘„å…¥æŒ‡æ ‡\n\n")
            f.write(f"- æ€»äº‹ä»¶æ•°: {len(tox_events)}\n")
            f.write(f"- ç´¯ç§¯æƒé‡: {sum(e['weight'] for e in tox_events)}\n\n")

            f.write("## Yè½´ï¼šæ”¿æ²»æš´è™æŒ‡æ ‡\n\n")
            f.write(f"- æ€»äº‹ä»¶æ•°: {len(tyr_events)}\n")
            f.write(f"- ç´¯ç§¯åˆ†æ•°: {sum(e['score'] for e in tyr_events)}\n\n")

            f.write("## ä¸å˜‰é–åˆæœŸå¯¹æ¯”\n\n")
            f.write("ï¼ˆå½’ä¸€åŒ–åˆ°æ¯10ä¸‡å­—ï¼‰\n\n")
            f.write("| æŒ‡æ ‡ | å˜‰é–åˆæœŸ | å£¬å¯…æ—¶æœŸ | å˜åŒ–ç‡ |\n")
            f.write("|------|----------|----------|--------|\n")

            early, current = comparison
            normalize = 100000

            metrics = {
                'toxicity_events': 'ä¿®é“æ´»åŠ¨é¢‘æ¬¡',
                'toxicity_weight': 'ç´¯ç§¯æ¯’æ€§æƒé‡',
                'tyranny_events': 'æš´è™äº‹ä»¶é¢‘æ¬¡',
                'tyranny_score': 'ç´¯ç§¯æš´è™åˆ†æ•°'
            }

            for key, label in metrics.items():
                early_val = early[key] / early['chars'] * normalize
                current_val = current[key] / current['chars'] * normalize

                if early_val > 0:
                    change = ((current_val - early_val) / early_val) * 100
                    change_str = f"+{change:.1f}%" if change > 0 else f"{change:.1f}%"
                else:
                    change_str = "N/A"

                f.write(f"| {label} | {early_val:.1f} | {current_val:.1f} | {change_str} |\n")

        print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file


def main():
    """ä¸»ç¨‹åº"""
    print("="*60)
    print("å£¬å¯…å®«å˜ä¸“é¡¹åˆ†æå·¥å…·")
    print("éªŒè¯'ç‚¼ä¸¹â†’é‡é‡‘å±ä¸­æ¯’â†’æš´è™â†’å®«å˜'å› æœé“¾")
    print("="*60)

    data_file = "jiajing_data_from_pdf/renyin_gongbian_era_vol228-276.txt"

    analyzer = RenyinAnalyzer(data_file)

    # æ­¥éª¤1: æœç´¢å®«å˜å…³é”®è¯
    palace_keywords = analyzer.search_palace_incident_keywords()

    # æ­¥éª¤2&3: æå–æ¯’æ€§å’Œæš´è™æŒ‡æ ‡ï¼ˆåœ¨å¯¹æ¯”å‡½æ•°ä¸­å®Œæˆï¼‰

    # æ­¥éª¤4: å¯¹æ¯”åˆ†æ
    early_data, current_data = analyzer.compare_with_early_jiajing()

    # é‡æ–°æå–ç”¨äºæŠ¥å‘Š
    tox_events = analyzer.extract_toxicity_enhanced()
    tyr_events = analyzer.extract_tyranny_enhanced()

    # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
    report_file = analyzer.generate_renyin_report(
        palace_keywords,
        tox_events,
        tyr_events,
        (early_data, current_data)
    )

    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆ!")
    print("="*60)
    print(f"\nğŸ“Š æ ¸å¿ƒæ•°æ®:")
    print(f"  - ä¿®é“æ´»åŠ¨: {len(tox_events)}æ¬¡ (ç´¯ç§¯æƒé‡{sum(e['weight'] for e in tox_events)})")
    print(f"  - æš´è™äº‹ä»¶: {len(tyr_events)}æ¬¡ (ç´¯ç§¯åˆ†æ•°{sum(e['score'] for e in tyr_events)})")
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
