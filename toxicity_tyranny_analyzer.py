# -*- coding: utf-8 -*-
"""
å˜‰é–å¸é‡é‡‘å±ä¸­æ¯’ä¸æ”¿æ²»æš´è™ç›¸å…³æ€§åˆ†æå·¥å…·
Toxicity-Tyranny Correlation Analyzer

ç ”ç©¶å‡è®¾ï¼š
å˜‰é–å¸çš„ä¸¹è¯æ‘„å…¥ï¼ˆé‡é‡‘å±ä¸­æ¯’ï¼‰ä¸æ”¿æ²»æš´è™è¡Œä¸ºå­˜åœ¨æ—¶é—´æ»åç›¸å…³æ€§
"""
import sys
import re
from pathlib import Path
from collections import defaultdict
import json

if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')


class ToxicityTyrannyAnalyzer:
    """æ¯’æ€§-æš´è™ç›¸å…³æ€§åˆ†æå™¨"""

    def __init__(self, data_file):
        self.data_file = Path(data_file)
        self.content = ""
        self.events = []
        self.load_data()

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        if not self.data_file.exists():
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {self.data_file}")
            return False

        with open(self.data_file, 'r', encoding='utf-8') as f:
            self.content = f.read()

        print(f"âœ“ å·²åŠ è½½æ•°æ®: {len(self.content):,}å­—")
        return True

    def extract_toxicity_indicators(self):
        """
        æå–Xè½´ï¼šé‡é‡‘å±æ‘„å…¥/ä¿®é“æ´»åŠ¨æŒ‡æ ‡

        è¿”å›: [(ä½ç½®, æ—¥æœŸ, å…³é”®è¯, æƒé‡), ...]
        """
        print("\n" + "="*60)
        print("ç¬¬ä¸€æ­¥ï¼šæ„å»ºé‡é‡‘å±æ‘„å…¥æŒ‡æ•°ï¼ˆXè½´ï¼‰")
        print("="*60)

        # å®šä¹‰å…³é”®è¯åŠå…¶æƒé‡
        keywords = {
            # ç›´æ¥ä»£ç†ï¼ˆé«˜æƒé‡ï¼‰
            'è¿›ä¸¹': 10, 'èµè¯': 10, 'çº¢é“…': 10, 'ç§‹çŸ³': 10,
            'é™¶ä»²æ–‡': 8, 'é‚µå…ƒèŠ‚': 8,

            # é—´æ¥ä»£ç†ï¼ˆä¸­æƒé‡ - ä¿®é“æ´»åŠ¨ï¼‰
            'é†®': 5, 'ç¥·': 5, 'ç¥€': 3, 'æ–‹': 3, 'é›·å›': 5,
            'å»ºé†®': 7, 'ç¥·ç¥€': 6,

            # ç—…ç†ååº”ï¼ˆä¸­æ¯’ç—‡çŠ¶ï¼‰
            'ä¸è±«': 4, 'ç”šè‡³': 2, 'å¿ƒæ‚¸': 6, 'ä¸èƒ½è§†æœ': 5
        }

        toxicity_events = []

        for keyword, weight in keywords.items():
            pos = 0
            count = 0

            while True:
                pos = self.content.find(keyword, pos)
                if pos == -1:
                    break

                # æå–ä¸Šä¸‹æ–‡
                start = max(0, pos - 200)
                end = min(len(self.content), pos + len(keyword) + 200)
                context = self.content[start:end]

                # æå–æ—¥æœŸ
                date_match = re.search(r'(å˜‰é–\w+å¹´\w+æœˆ\w+)', context)
                date = date_match.group(1) if date_match else "æœªçŸ¥æ—¥æœŸ"

                toxicity_events.append({
                    'position': pos,
                    'date': date,
                    'keyword': keyword,
                    'weight': weight,
                    'context': context
                })

                count += 1
                pos += len(keyword)

            if count > 0:
                print(f"  [{keyword}]: {count}æ¬¡ (æƒé‡={weight})")

        # æŒ‰ä½ç½®æ’åº
        toxicity_events.sort(key=lambda x: x['position'])

        print(f"\nâœ“ å…±æ‰¾åˆ° {len(toxicity_events)} ä¸ªé‡é‡‘å±/ä¿®é“æ´»åŠ¨æŒ‡æ ‡")
        return toxicity_events

    def extract_tyranny_indicators(self):
        """
        æå–Yè½´ï¼šæ”¿æ²»æš´è™æŒ‡æ ‡

        è¿”å›: [(ä½ç½®, æ—¥æœŸ, å…³é”®è¯, æš´è™åˆ†æ•°), ...]
        """
        print("\n" + "="*60)
        print("ç¬¬äºŒæ­¥ï¼šæ„å»ºæ”¿æ²»æš´è™æŒ‡æ•°ï¼ˆYè½´ï¼‰")
        print("="*60)

        # å®šä¹‰å…³é”®è¯åŠå…¶æš´è™åˆ†æ•°
        keywords = {
            # ä¸€çº§æš´è™ï¼ˆè‡´æ­»/è‚‰ä½“ä¼¤å®³ - 10åˆ†ï¼‰
            'å»·æ–': 10, 'æ¯™äºæ–ä¸‹': 10, 'å¼ƒå¸‚': 10, 'æ–©': 10,
            'ç»': 10, 'ä¸‹è¯ç‹±': 9, 'é”¦è¡£å«': 7,

            # äºŒçº§æš´è™ï¼ˆæ”¿æ²»æ¸…æ´— - 5åˆ†ï¼‰
            'å‰Šç±': 5, 'ä¸ºæ°‘': 4, 'è‡´ä»•': 2, 'è¤«å¤º': 6, 'åˆ‡è´£': 5,
            'ç½¢é»œ': 4, 'è´¬è°ª': 5, 'æˆè¾¹': 6,

            # ä¸‰çº§æš´è™ï¼ˆæƒ…ç»ªå®£æ³„ - 2åˆ†ï¼‰
            'éœ‡æ€’': 3, 'å¤§æ€’': 4, 'æ·è¡¨': 3, 'å±é€€': 2
        }

        tyranny_events = []

        for keyword, score in keywords.items():
            pos = 0
            count = 0

            while True:
                pos = self.content.find(keyword, pos)
                if pos == -1:
                    break

                # æå–ä¸Šä¸‹æ–‡
                start = max(0, pos - 200)
                end = min(len(self.content), pos + len(keyword) + 200)
                context = self.content[start:end]

                # æå–æ—¥æœŸ
                date_match = re.search(r'(å˜‰é–\w+å¹´\w+æœˆ\w+)', context)
                date = date_match.group(1) if date_match else "æœªçŸ¥æ—¥æœŸ"

                tyranny_events.append({
                    'position': pos,
                    'date': date,
                    'keyword': keyword,
                    'score': score,
                    'context': context
                })

                count += 1
                pos += len(keyword)

            if count > 0:
                print(f"  [{keyword}]: {count}æ¬¡ (åˆ†æ•°={score})")

        # æŒ‰ä½ç½®æ’åº
        tyranny_events.sort(key=lambda x: x['position'])

        print(f"\nâœ“ å…±æ‰¾åˆ° {len(tyranny_events)} ä¸ªæš´è™äº‹ä»¶æŒ‡æ ‡")
        return tyranny_events

    def analyze_correlation(self, toxicity_events, tyranny_events):
        """
        åˆ†æXä¸Yçš„ç›¸å…³æ€§
        """
        print("\n" + "="*60)
        print("ç¬¬ä¸‰æ­¥ï¼šç›¸å…³æ€§åˆ†æ")
        print("="*60)

        if not toxicity_events or not tyranny_events:
            print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")
            return

        # æŒ‰æ—¥æœŸåˆ†ç»„
        toxicity_by_date = defaultdict(list)
        for event in toxicity_events:
            toxicity_by_date[event['date']].append(event)

        tyranny_by_date = defaultdict(list)
        for event in tyranny_events:
            tyranny_by_date[event['date']].append(event)

        # è®¡ç®—æ¯ä¸ªæ—¥æœŸçš„ç´¯ç§¯åˆ†æ•°
        print("\nğŸ“Š æŒ‰æ—¥æœŸçš„æ¯’æ€§-æš´è™åˆ†æ•°å¯¹ç…§:\n")

        all_dates = sorted(set(list(toxicity_by_date.keys()) + list(tyranny_by_date.keys())))

        results = []
        for date in all_dates:
            tox_score = sum(e['weight'] for e in toxicity_by_date.get(date, []))
            tyr_score = sum(e['score'] for e in tyranny_by_date.get(date, []))

            if tox_score > 0 or tyr_score > 0:
                results.append({
                    'date': date,
                    'toxicity': tox_score,
                    'tyranny': tyr_score,
                    'tox_events': len(toxicity_by_date.get(date, [])),
                    'tyr_events': len(tyranny_by_date.get(date, []))
                })

        # æ˜¾ç¤ºå‰20æ¡
        print(f"{'æ—¥æœŸ':<20} {'æ¯’æ€§åˆ†æ•°':<10} {'æš´è™åˆ†æ•°':<10} {'æ¯’æ€§äº‹ä»¶':<10} {'æš´è™äº‹ä»¶':<10}")
        print("-" * 70)

        for i, r in enumerate(results[:20]):
            print(f"{r['date']:<20} {r['toxicity']:<10} {r['tyranny']:<10} {r['tox_events']:<10} {r['tyr_events']:<10}")

        if len(results) > 20:
            print(f"\n... è¿˜æœ‰ {len(results) - 20} æ¡è®°å½•æœªæ˜¾ç¤º")

        return results

    def find_high_correlation_cases(self, toxicity_events, tyranny_events, window=100000):
        """
        æŸ¥æ‰¾é«˜åº¦ç›¸å…³çš„æ¡ˆä¾‹

        window: ä½ç½®çª—å£å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œç”¨äºåˆ¤æ–­äº‹ä»¶çš„"æ—¶é—´æ¥è¿‘åº¦"
        """
        print("\n" + "="*60)
        print("ç¬¬å››æ­¥ï¼šé«˜åº¦ç›¸å…³æ¡ˆä¾‹æŒ–æ˜")
        print("="*60)

        print(f"\nâ±ï¸ æ—¶é—´çª—å£: {window:,}å­—ç¬¦ (çº¦ç­‰äºå‡ å¤©åˆ°å‡ å‘¨)\n")

        high_corr_cases = []

        # å¯¹æ¯ä¸ªä¿®é“æ´»åŠ¨ï¼ŒæŸ¥æ‰¾å…¶åwindowèŒƒå›´å†…çš„æš´è™äº‹ä»¶
        for tox_event in toxicity_events:
            tox_pos = tox_event['position']

            # æŸ¥æ‰¾çª—å£å†…çš„æš´è™äº‹ä»¶
            nearby_tyranny = [
                tyr for tyr in tyranny_events
                if tox_pos < tyr['position'] < tox_pos + window
            ]

            if nearby_tyranny:
                total_tyranny_score = sum(t['score'] for t in nearby_tyranny)

                # åªè®°å½•é«˜åˆ†æ¡ˆä¾‹
                if tox_event['weight'] >= 5 and total_tyranny_score >= 5:
                    high_corr_cases.append({
                        'tox_event': tox_event,
                        'tyranny_events': nearby_tyranny,
                        'total_tyranny': total_tyranny_score,
                        'distance': nearby_tyranny[0]['position'] - tox_pos
                    })

        # æŒ‰æš´è™æ€»åˆ†æ’åº
        high_corr_cases.sort(key=lambda x: x['total_tyranny'], reverse=True)

        print(f"ğŸ” å‘ç° {len(high_corr_cases)} ä¸ªé«˜ç›¸å…³æ€§æ¡ˆä¾‹\n")

        # æ˜¾ç¤ºå‰10ä¸ªæ¡ˆä¾‹
        for i, case in enumerate(high_corr_cases[:10], 1):
            tox = case['tox_event']
            print(f"ã€æ¡ˆä¾‹ {i}ã€‘")
            print(f"  æ¯’æ€§äº‹ä»¶: [{tox['keyword']}] (æƒé‡={tox['weight']}) @ {tox['date']}")
            print(f"  éšåå‘ç”Ÿçš„æš´è™äº‹ä»¶ ({len(case['tyranny_events'])}ä¸ªï¼Œæ€»åˆ†={case['total_tyranny']}):")
            for tyr in case['tyranny_events'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"    - [{tyr['keyword']}] (åˆ†æ•°={tyr['score']}) @ {tyr['date']}")
            print(f"  æ—¶é—´é—´éš”: çº¦{case['distance']:,}å­—ç¬¦")
            print()

        return high_corr_cases

    def generate_report(self, toxicity_events, tyranny_events, correlation_results, high_corr_cases):
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ç¬¬äº”æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print("="*60)

        report = {
            'metadata': {
                'data_source': str(self.data_file),
                'total_chars': len(self.content),
                'analysis_date': '2026-01-02'
            },
            'toxicity_summary': {
                'total_events': len(toxicity_events),
                'total_weight': sum(e['weight'] for e in toxicity_events),
                'keywords': list(set(e['keyword'] for e in toxicity_events))
            },
            'tyranny_summary': {
                'total_events': len(tyranny_events),
                'total_score': sum(e['score'] for e in tyranny_events),
                'keywords': list(set(e['keyword'] for e in tyranny_events))
            },
            'correlation': {
                'date_count': len(correlation_results) if correlation_results else 0,
                'high_corr_cases': len(high_corr_cases)
            }
        }

        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = Path("toxicity_tyranny_analysis.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {json_file}")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_file = Path("å˜‰é–å¸æ¯’æ€§-æš´è™ç›¸å…³æ€§åˆ†æ.md")
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# å˜‰é–å¸é‡é‡‘å±ä¸­æ¯’ä¸æ”¿æ²»æš´è™ç›¸å…³æ€§åˆ†ææŠ¥å‘Š\n\n")
            f.write("## ç ”ç©¶å‡è®¾\n\n")
            f.write("å˜‰é–å¸çš„ä¸¹è¯æ‘„å…¥ï¼ˆé‡é‡‘å±ä¸­æ¯’ï¼‰ä¸æ”¿æ²»æš´è™è¡Œä¸ºå­˜åœ¨æ—¶é—´æ»åç›¸å…³æ€§\n\n")

            f.write("## æ•°æ®æ¥æº\n\n")
            f.write(f"- æ–‡ä»¶: {self.data_file}\n")
            f.write(f"- å­—æ•°: {len(self.content):,}å­—\n")
            f.write(f"- è¦†ç›–: å˜‰é–å…ƒå¹´-ä¸‰å¹´+\n\n")

            f.write("## Xè½´ï¼šé‡é‡‘å±æ‘„å…¥/ä¿®é“æ´»åŠ¨æŒ‡æ•°\n\n")
            f.write(f"- æ€»äº‹ä»¶æ•°: {len(toxicity_events)}\n")
            f.write(f"- ç´¯ç§¯æƒé‡: {sum(e['weight'] for e in toxicity_events)}\n")
            f.write(f"- å…³é”®è¯: {', '.join(set(e['keyword'] for e in toxicity_events))}\n\n")

            f.write("## Yè½´ï¼šæ”¿æ²»æš´è™æŒ‡æ•°\n\n")
            f.write(f"- æ€»äº‹ä»¶æ•°: {len(tyranny_events)}\n")
            f.write(f"- ç´¯ç§¯åˆ†æ•°: {sum(e['score'] for e in tyranny_events)}\n")
            f.write(f"- å…³é”®è¯: {', '.join(set(e['keyword'] for e in tyranny_events))}\n\n")

            f.write("## é«˜åº¦ç›¸å…³æ¡ˆä¾‹\n\n")
            for i, case in enumerate(high_corr_cases[:10], 1):
                tox = case['tox_event']
                f.write(f"### æ¡ˆä¾‹ {i}\n\n")
                f.write(f"**æ¯’æ€§äº‹ä»¶**: [{tox['keyword']}] (æƒé‡={tox['weight']}) @ {tox['date']}\n\n")
                f.write(f"**éšåçš„æš´è™äº‹ä»¶** ({len(case['tyranny_events'])}ä¸ªï¼Œæ€»åˆ†={case['total_tyranny']}):\n\n")
                for tyr in case['tyranny_events']:
                    f.write(f"- [{tyr['keyword']}] (åˆ†æ•°={tyr['score']}) @ {tyr['date']}\n")
                f.write(f"\n**æ—¶é—´é—´éš”**: çº¦{case['distance']:,}å­—ç¬¦\n\n")
                f.write("---\n\n")

        print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_file}")

        return report


def main():
    """ä¸»ç¨‹åº"""
    print("="*60)
    print("å˜‰é–å¸é‡é‡‘å±ä¸­æ¯’ä¸æ”¿æ²»æš´è™ç›¸å…³æ€§åˆ†æå·¥å…·")
    print("Toxicity-Tyranny Correlation Analyzer")
    print("="*60)

    # åŠ è½½æ•°æ®
    data_file = "jiajing_data_from_pdf/complete_vol1-45.txt"
    analyzer = ToxicityTyrannyAnalyzer(data_file)

    # ç¬¬ä¸€æ­¥ï¼šæå–Xè½´ï¼ˆé‡é‡‘å±æ‘„å…¥ï¼‰
    toxicity_events = analyzer.extract_toxicity_indicators()

    # ç¬¬äºŒæ­¥ï¼šæå–Yè½´ï¼ˆæ”¿æ²»æš´è™ï¼‰
    tyranny_events = analyzer.extract_tyranny_indicators()

    # ç¬¬ä¸‰æ­¥ï¼šç›¸å…³æ€§åˆ†æ
    correlation_results = analyzer.analyze_correlation(toxicity_events, tyranny_events)

    # ç¬¬å››æ­¥ï¼šé«˜åº¦ç›¸å…³æ¡ˆä¾‹æŒ–æ˜
    high_corr_cases = analyzer.find_high_correlation_cases(toxicity_events, tyranny_events, window=50000)

    # ç¬¬äº”æ­¥ï¼šç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report(toxicity_events, tyranny_events, correlation_results, high_corr_cases)

    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œæˆ!")
    print("="*60)
    print("\nğŸ“Š æ ¸å¿ƒå‘ç°:")
    print(f"  - é‡é‡‘å±/ä¿®é“äº‹ä»¶: {report['toxicity_summary']['total_events']}æ¬¡")
    print(f"  - æ”¿æ²»æš´è™äº‹ä»¶: {report['tyranny_summary']['total_events']}æ¬¡")
    print(f"  - é«˜åº¦ç›¸å…³æ¡ˆä¾‹: {report['correlation']['high_corr_cases']}ä¸ª")
    print("\nğŸ’¡ æç¤º:")
    print("  - æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: å˜‰é–å¸æ¯’æ€§-æš´è™ç›¸å…³æ€§åˆ†æ.md")
    print("  - æŸ¥çœ‹æ•°æ®æ–‡ä»¶: toxicity_tyranny_analysis.json")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
